{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K861UBwvMHaz",
        "outputId": "cc4d2f3b-f39f-47b3-e4c1-2835d563c27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Collecting importlib-metadata<6.0.0,>=5.1.0\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.6.0\n",
            "    Uninstalling importlib-metadata-6.6.0:\n",
            "      Successfully uninstalled importlib-metadata-6.6.0\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 syllables-1.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen\n",
            "Successfully installed pyphen-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install textblob syllables\n",
        "!pip install pyphen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import syllables\n",
        "from syllables import estimate\n"
      ],
      "metadata": {
        "id": "j0aXm4O9NaHv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from syllables import estimate \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o29eeQiYNGMW",
        "outputId": "1e2c2a61-8eaf-4cc4-a1bb-e0d05f5b10bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyphen\n",
        "\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "\n",
        "def syllables(word):\n",
        "    \"\"\"\n",
        "    Count the number of syllables in a word using Pyphen.\n",
        "    \"\"\"\n",
        "    return len(dic.inserted(word).split('-'))"
      ],
      "metadata": {
        "id": "ueJ3ITxTMhAX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# download the stop words list\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# read the Excel file containing URLs\n",
        "df = pd.read_excel(\"Input.xlsx\")\n",
        "\n",
        "# create lists to store the results\n",
        "titles = []\n",
        "texts = []\n",
        "positive_scores = []\n",
        "negative_scores = []\n",
        "polarity_scores = []\n",
        "subjectivity_scores = []\n",
        "fog_indexes = []\n",
        "average_sentence_lengths = []\n",
        "percentage_complex_word = []\n",
        "average_word_lengths = []\n",
        "syllable_counts = []\n",
        "personal_pronouns_counts = []\n",
        "word_counts = []\n",
        "\n",
        "\n",
        "# iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url = row[\"URL\"]\n",
        "    \n",
        "    # fetch the webpage content using urllib\n",
        "    try:\n",
        "        html = urllib.request.urlopen(url).read()\n",
        "    except:\n",
        "        print(f\"Invalid URL: {url}\")\n",
        "        continue\n",
        "\n",
        "    # parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # extract the article title\n",
        "    title = soup.title.string\n",
        "    titles.append(title)\n",
        "\n",
        "    # extract the article text\n",
        "    text = \"\"\n",
        "    for article in soup.find_all(\"article\"):\n",
        "        for p in article.find_all(\"p\"):\n",
        "            text += p.get_text()\n",
        "    texts.append(text)\n",
        "\n",
        "#POSITIVE\n",
        "\n",
        "    positive_words = \"/content/positive-words.txt\"\n",
        "    with open(positive_words, \"r\") as f:\n",
        "        positive_words = f.read().splitlines()\n",
        "\n",
        "    negative_words = \"/content/negative-words.txt\"\n",
        "    \n",
        "    with open('/content/negative-words.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        negative_words = set(f.read().split())\n",
        "    \n",
        "\n",
        "\n",
        "    # calculate positive and negative scores\n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in positive_words:\n",
        "            positive_score += 1\n",
        "        elif word in negative_words:\n",
        "            negative_score += 1\n",
        "    positive_scores.append(positive_score)\n",
        "    negative_scores.append(-1 * negative_score)\n",
        "\n",
        "    # calculate polarity and subjectivity scores using TextBlob\n",
        "    blob = TextBlob(text)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / ((len(words) - len(stop_words)) + 0.000001)\n",
        "    polarity_scores.append(polarity_score)\n",
        "    subjectivity_scores.append(subjectivity_score)\n",
        "\n",
        "    # calculate fog index and average sentence length\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "    complex_words = []\n",
        "    for word in words:\n",
        "        syllable_count = 0  # initialize syllable_count with a default value\n",
        "        if len(word) > 2 and word not in stop_words and re.match(\"^[a-zA-Z0-9_]*$\", word): syllable_count = estimate(word)\n",
        "        if syllable_count > 2:\n",
        "            complex_words.append(word)\n",
        "\n",
        "    total_complex_words = len(complex_words)\n",
        "    average_sentence_length = total_words / total_sentences\n",
        "    percentage_complex_words = (total_complex_words / total_words) * 100\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "    fog_indexes.append(fog_index)\n",
        "    average_sentence_lengths.append(average_sentence_length)\n",
        "    percentage_complex_word.append(percentage_complex_words)\n",
        "\n",
        "    # create a dictionary to store the results for each URL\n",
        "results = {\"URL_ID\": [], \"URL\": [], \"Positive Score\": [], \"Negative Score\": [], \"Polarity Score\": [], \"Subjectivity Score\": [], \"Average Number of Words Per Sentence\": [], \"Complex Word Count\": [], \"Word Count\": [], \"Syllable Count Per Word\": [], \"Personal Pronouns\": [], \"Average Word Length\": []}\n",
        "\n",
        "# iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url = row[\"URL\"]\n",
        "    \n",
        "    # fetch the webpage content using urllib\n",
        "    try:\n",
        "        html = urllib.request.urlopen(url).read()\n",
        "    except:\n",
        "        print(f\"Invalid URL: {url}\")\n",
        "        continue\n",
        "\n",
        "    # parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # extract the article text\n",
        "    text = \"\"\n",
        "    for article in soup.find_all(\"article\"):\n",
        "        for p in article.find_all(\"p\"):\n",
        "            text += p.get_text()\n",
        "\n",
        "    # clean the text by removing stop words and punctuation\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text) if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # calculate the various scores and metrics\n",
        "    pscore = sum(1 for word in words if word in positive_words)\n",
        "    nscore = sum(1 for word in words if word in negative_words) * -1\n",
        "    ppscore = (pscore - nscore) / ((pscore + nscore) + 0.000001)\n",
        "    subjscore = (pscore + nscore) / (len(words) + 0.000001)\n",
        "    sent_count = len(nltk.sent_tokenize(text))\n",
        "    word_count = len(words)\n",
        "    avg_words_per_sent = word_count / sent_count\n",
        "    \n",
        "    complex_count = sum(1 for word in words if syllables(word) > 2)\n",
        "    syllable_count = sum(syllables(word) for word in words)\n",
        "    personal_pronouns = len(re.findall(r\"\\b(i|we|my|ours|us)\\b\", text, re.IGNORECASE))\n",
        "    avg_word_length = sum(len(word) for word in words) / (len(words) + 0.000001)\n",
        "\n",
        "    # append the results to the dictionary\n",
        "    results[\"URL_ID\"].append(row[\"URL_ID\"])\n",
        "    results[\"URL\"].append(url)\n",
        "    results[\"Positive Score\"].append(pscore)\n",
        "    results[\"Negative Score\"].append(nscore)\n",
        "    results[\"Polarity Score\"].append(ppscore)\n",
        "    results[\"Subjectivity Score\"].append(subjscore)\n",
        "    results[\"Average Number of Words Per Sentence\"].append(avg_words_per_sent)\n",
        "    results[\"Complex Word Count\"].append(complex_count)\n",
        "    results[\"Word Count\"].append(word_count)\n",
        "    results[\"Syllable Count Per Word\"].append(syllable_count)\n",
        "    results[\"Personal Pronouns\"].append(personal_pronouns)\n",
        "    results[\"Average Word Length\"].append(avg_word_length)\n",
        "\n",
        "# create a DataFrame from the results dictionary\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# save the results to a Excel file\n",
        "results_df.to_excel(\"Output10.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoSKKjrQMyhS",
        "outputId": "a6b28b04-1e12-41b5-972c-9a1c22fca234"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Invalid URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Invalid URL: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n",
            "Invalid URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Invalid URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Invalid URL: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
          ]
        }
      ]
    }
  ]
}